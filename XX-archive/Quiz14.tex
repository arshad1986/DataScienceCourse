%----------------------------------------------------------------------%
\subsection*{Question 1}
Why is a Deep Belief Network not a Boltzmann Machine ?
% Your Answer		Score	Explanation
A DBN is not a probabilistic model of the data.			
A DBN does not have hidden units.			
Some edges in a DBN are directed.			
All edges in a DBN are directed.	Inorrect	0.00	A DBN has undirected edges in the top-layer RBM.
Total		0.00 / 1.00	
%----------------------------------------------------------------------%
\subsection*{Question 2}
Brian looked at the direction of arrows in a DBN and was surprised to find that the data is at the "output". "Where is the input ?!", he exclaimed, "How will I give input to this model and get all those cool features?" In this context, which of the following statements are true? Check all that apply.
% Your Answer		Score	Explanation
In order to get features h given some data v, he must perform inference to find out P(h|v). There is an easy approximate way of doing this, just traverse the arrows in the opposite direction.	Correct	0.50	Traversing arrows in the opposite direction is an approximate inference procedure.
A DBN is a generative model of the data and cannot be used to generate features for any given input. It can only be used to get features for data that was generated by the model.	Correct	0.50	This is not true. A DBN can be used to generate features for any given data vector by doing inference.
A DBN is a generative model of the data, which means that, its arrows define a way of generating data from a probability distribution, so there is no "input".	Inorrect	0.00	Unlike a feed-forward neural net, a DBN is not a mapping from inputs to outputs. It is a probabilistic generative model of the data.
In order to get features h given some data v, he must perform inference to find out P(h|v). There is an easy exact way of doing this, just traverse the arrows in the opposite direction.	Correct	0.50	Traversing arrows in the opposite direction is an approximate inference procedure.
Total		1.50 / 2.00
%----------------------------------------------------------------------%	
\subsection*{Question 3}
In which of the following cases is pretraining likely to help the most (compared to training a neural net from random initialization) ?
% Your Answer		Score	Explanation
A dataset of images is to be classified into 100 semantic classes. There are only 1,000 labelled images but 100 million unlabelled ones are available from the internet.	Correct	1.00	The small labelled set of images is unlikely to have enough information to learn good features. The unlabelled images can be used to do pretraining for learning features.
A dataset of binary pixel images which are to be classified based on parity, i.e., if the sum of pixels is even the image has label 0, otherwise it has label 1.			
A speech dataset with 10 billion labelled training examples.			
A dataset of images is to be classified into 100 semantic classes. Fortunately, there are 100 million labelled training examples.			
Total		1.00 / 1.00	
%----------------------------------------------------------------------%
\subsection*{Question 4}
Why does pretraining help more when the network is deep ?
% Your Answer		Score	Explanation
During backpropagation in very deep nets, the lower level layers get very small gradients, making it hard to learn good low-level features. Since pretraining starts those low-level features off at a good point, there is a big win.	Inorrect	0.00	Lower level layers can get very small gradients, especially if saturating hidden units are used (such as logistic or tanh units). Pretraining can initialize the weights in a proper region of weight space so that the features don't have to start learning from scratch.
As nets get deeper, contrastive divergence objective used during pretraining gets closer to the classification objective.	Correct	0.50	Contrastive divergence has nothing to do with the classification objective.
Deeper nets have more parameters than shallow ones and they overfit easily. Therefore, initializing them sensibly is important.	Inorrect	0.00	More parameters means that the model can find ingenious ways of overfitting by learning features that don't generalize well. Pretraining can initialize the weights in a proper region of weight space so that the features learned are not too bad.
Backpropagation algorithm cannot give accurate gradients for very deep networks. So it is important to have good initializtions, especially, for the lower layers.	Inorrect	0.00	Backpropgation gives exact gradients.
Total		0.50 / 2.00	
%----------------------------------------------------------------------%
\subsection*{Question 5}
The energy function for binary RBMs goes by 
E(v,h)=−\sumivibi−\sumjhjaj−\sumi,jviWijhj 
When modeling real-valued data (i.e., when v is a real-valued vector not a binary one) we change it to 
E(v,h)=\sumi(vi−bi)22\sigma2i−\sumjhjaj−\sumi,jvi\sigmaiWijhj 
Why can't we still use the same old one ?
% Your Answer		Score	Explanation
If the model assigns an energy e1 to state v1,h, and e2 to state v2,h, then it would assign energy (e1+e2)/2 to state (v1+v2)/2,h. This does not make sense for the kind of distributions we usually want to model.	Correct	0.50	Suppose v1 and v2 represent two images. We would like e1 and e2 to be small. This makes the energy of the average image low, but the average of two images would not look like a natural image and should not have low energy.
Probability distributions over real-valued data can only be modeled by having a conditional Gaussian distribution over them. So we have to use a quadratic term.	Correct	0.50	This is not true. Distributions over real-valued data can be modelled by a multitude of distributions or combinations thereof. The choice of Gaussian distribution is arbitrary.
If we continue to use the same one, then in general, there will be infinitely many v's and h's such that, E(v,h) will be infinitely small (close to −∞). The probability distribution resulting from such an energy function is not useful for modeling real data.	Correct	0.50	If some bi0 if vi→∞, then E→−∞. So the Boltzmann distribution based on this energy function would behave in weird ways.
If we use the old one, the real-valued vectors would end up being constrained to be binary.	Correct	0.50	The energy function does not impose any such constraints.
Total		2.00 / 2.00	
