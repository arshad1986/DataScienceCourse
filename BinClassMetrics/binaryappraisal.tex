
\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode"CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 201113:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Kevin O'Brien} \chead{Week 8} %\input{tcilatex}

%http://www.electronics.dit.ie/staff/ysemenova/Opto2/CO_IntroLab.pdf
\begin{document}
\section{Binary Classification}
\subsection{What Is Classification}
Classification is the problem of identifying to which of a set of categories (sub-populations) a new
observation belongs, on the basis of a training set of data containing observations (or instances)
whose category membership is known. Binary Classification is the task of classifying the members of
a given set of objects into two groups on the basis if them having a particular set of characteristics.
\begin{itemize}

\item  To train (create) a classifier, the fitting function estimates the parameters of a Gaussian
distribution for each class.
\item  To predict the classes of new data, the trained classifier finds the class with the smallest
misclassification cost.
Binary Classification Prediction Procedure
Binary Variabble: Positive or Negative
Four Possible Outcomes from Classification Procedure:
\item  TN / True Negative: Case was actually negative and was also predicted negative
(CORRECT).
\item  TP / True Positive: Case was actually positive and was also predicted positive (CORRECT).
\item  FN / False Negative: Case was actually positive but was predicted negative
(WRONG).
\item  FP / False Positive: Case was actually negative but was predicted positive (WRONG).
Remark : We will use this notation to specify the number of cases in each category also: i.e. TP
= 5000 means 5000 True Positives.

\item  Accuracy is not a reliable metric for the real performance of a classification system, because
it will yield misleading results if the data set is unbalanced (that is, when the number of
samples in different classes vary greatly).
\item  For example, if there were 95 cats and only 5 dogs in the data set, the classifier could
easily be biased into classifying all the samples as cats. The overall accuracy would be
95%, but in practice the classifier would have a 100% recognition rate for the cat class
but a 0% recognition rate for the dog class.
1
Predicted Predicted
Negative Positive
Actual State: Negative TN FP
Actual State: Positive FN TP
False Positive and False Negative Error
\item  A false positive error, commonly called a “false alarm“, is a result that indicates a given
condition has been fulfilled, when it actually has not been fulfilled. A false positive error is
a Type I error.
\item  A false negative error is where a test result indicates that a condition failed, while it actually
was successful. A false negative error is a Type II error.
Medical Testing example Defining true/false positives
In general, Positive = identified and negative = rejected. Therefore:
TN True negative = Healthy people correctly identified as healthy (correctly rejected)
FP False positive = Healthy people incorrectly identified as sick (incorrectly identified)
FN False negative = Sick people incorrectly identified as healthy. incorrectly rejected
TP True positive = Sick people correctly diagnosed as sick (correctly identified)
Types I and II Error (For Later)
\item  A Type I error is the incorrect rejection of a true null hypothesis.
\item  A Type II error is the failure to reject a false null hypothesis.
\item  A Type I error is a false positive. Usually a type I error leads one to conclude that a thing
or relationship exists when really it doesn’t.
\item  A type II error is a false negative.
Null hypothesis (H0) is true Null hypothesis (H0) is false
Reject Type I error Correct Outcome
null hypothesis False positive True positive
Fail to reject Correct Outcome Type II error
null hypothesis True negative False negative
\end{itemize}

Sensitivity and Specificity
Sensitivity and specificity are measures of the performance of a binary classification test.
\item  Sensitivity (also called the true positive rate, or the recall rate) measures the proportion
of actual positives which are correctly identified as such (e.g. the percentage of sick people
who are correctly identified as having the condition).
Sensitivity (Recall) = T P
T P + F N
\item  Examples: Sensitivity (TPR), also known as recall, is the proportion of people that tested
positive (TP) of all the people that actually are positive (TP+FN).
– It can be seen as the probability that the test is positive given that the patient is sick.
– With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of
the factory quality control, the fewer faulty products go to the market).
– (Remark: We will use the terms Sensitivity and Recall interchangeably. Sensitivity is
more commonly used in a medical context, while recall is more commonly used in data
science.)
\item  Specificity measures the proportion of negatives which are correctly identified as such (e.g.
the percentage of healthy people who are correctly identified as not having the condition,
sometimes called the true negative rate).
Specificity = T N
T P + F N
– (Remark: Not commonly used in Data Sciences, and NOT a synonym for Precision)
\item  Examples: Specificity (TNR) is the proportion of people that tested negative (TN) of all
the people that actually are negative (TN+FP). As with sensitivity, it can be looked at as
the probability that the test result is negative given that the patient is not sick.
\item  With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, the
less money the factory loses by discarding good products instead of selling them).
\item  The relationship between sensitivity and specificity, as well as the performance of the classi-
fier, can be visualized and studied using the ROC curve (Which we shall see shortly).
\end{itemize}
\subsection{Precision}
number of items correctly labeled as belonging to the positive class) divided by the total number
of cases labeled as belonging to the positive class (i.e. the sum of true positives and false positives,
which are items incorrectly labeled as belonging to the class).
Precision = T P
T P + F P
(1)
3

\subsection{Accuracy, Recall and Precision: An Example}
Suppose we are designing a medical diagnosis system, and we have enlisted 10000 volunteers to
help us test it. Suppose there are 135 positive cases of an illness among the10,000 cases. You want
to predict which ones are positive, and you pick 265 to have a better chance of catching many of
the 135 positive cases. You record the IDs of your predictions, and when you get the actual results
you sum up how many times you were right or wrong.
Now count how many of the 10,000 cases fall in each category:
Predicted Negative Predicted Positive
Negative Cases TN: 9,700 FP: 165
Positive Cases FN: 35 TP: 100


1. What percent of your predictions were correct?
\item  The accuracy was (9,760+60) out of 10,000 = 98.00%
2. What percent of the positive cases did you catch?
\item  The recall was 100 out of 135 = 74.07%
3. What percent of positive predictions were correct?
\item  The precision was 100 out of 265 = 37.74%
4. What percent of negative predictions were correct?
\item  The specifity was 9700 out of 9735 = 99.64%





%-------------------------------------------------- %
\medskip
\subsection{Definitions (From Week 1)}
\textbf{Confusion Matrix} \\
The confusion
table is a table in which the rows are the observed categories of the dependent and
the columns are the predicted categories. When prediction is perfect all cases will lie on the
diagonal. The percentage of cases on the diagonal is the percentage of correct classifications. 

\textbf{Accuracy Rate}\\
The accuracy rate calculates the proportion ofobservations being allocated to the \textbf{correct} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Correct Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{TP + TN}{TP+FP+TN+FN}\]

\medskip

\noindent \textbf{Misclassification Rate}\\
The misclassification rate calculates the proportion ofobservations being allocated to the \textbf{incorrect} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Incorrect Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{FP + FN}{TP+FP+TN+FN}\]
\medskip

%=====================================================%


\subsection{Binary Classification}
\noindent \textbf{Defining True/False Positives}
In general, Positive = identified and negative = rejected. Therefore:

\begin{itemize}
	\item True positive = correctly identified
	
	\item False positive = incorrectly identified
	
	\item True negative = correctly rejected
	
	\item False negative = incorrectly rejected
\end{itemize}
\subsubsection*{Medical Testing Example:}
\begin{itemize}
	\item True positive = Sick people correctly diagnosed as sick
	
	\item False positive= Healthy people incorrectly identified as sick
	
	\item True negative = Healthy people correctly identified as healthy
	
	\item False negative = Sick people incorrectly identified as healthy.
\end{itemize}



%---------------------------------------------------------------------------------------%






%-------------------------------------------------- %
\newpage
\subsection{Definitions (From Week 1)}
\textbf{Confusion Matrix} \\
The confusion
table is a table in which the rows are the observed categories of the dependent and
the columns are the predicted categories. When prediction is perfect all cases will lie on the
diagonal. The percentage of cases on the diagonal is the percentage of correct classifications. 

\textbf{Accuracy Rate}\\
The accuracy rate calculates the proportion ofobservations being allocated to the \textbf{correct} group by the predictive model. It is calculated as follows:
\[ \frac{
\mbox{Number of Correct Classifications }}{\mbox{Total Number of Classifications }} \]

\[ = \frac{TP + TN}{TP+FP+TN+FN}\]

\medskip

\end{document}
