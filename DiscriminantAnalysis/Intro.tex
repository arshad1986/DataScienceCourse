% http://sites.stat.psu.edu/~ajw13/stat505/fa06/15_discrim/11_discrim_summary.htm
% http://core.ecu.edu/ofe/StatisticsResearch/SPSS%20Discriminant%20Function%20Analysis.pdf
% http://userwww.sfsu.edu/efc/classes/biol710/discrim/discrim.pdf

\begin{verbatim}
1) Purpose of DA
2) Eigen Values
3) Wilk's Lambda
\end{verbatim}

%----------------------------------------------------%
\subsection{What is Discriminant function analysis}
\begin{itemize}
\item It builds a predictive model for group membership
\item The model is composed of a discriminant function based on linear combinations of predictor variables.
\item Those predictor variables provide the best discrimination between groups.
\end{itemize}
%----------------------------------------------------%
\subsection{Purpose of Discriminant analysis}
\item to maximally separate the groups.
\item to determine the most parsimonious way to separate groups
\item to discard variables which are little related to group distinctions
\end{itemize}

%----------------------------------------------------%
\subsection{Summary}
We are interested in the relationship between a group of independent variables and one categorical variable. We would like to know how many dimensions we would need to express this relationship. Using this relationship, we can predict a classification based on the independent variables or assess how well the independent variables separate the categories in the classification.

%=====================================================================================%
\subsection{Relationship to MANOVA}
\begin{itemize}
\item Discriminant function analysis is multivariate analysis of variance (MANOVA)
reversed. In MANOVA, the independent variables are the groups and the
dependent variables are the predictors. In DA, the independent variables are the
predictors and the dependent variables are the groups. 
\item  DA is usually used to predict membership in naturally occurring
groups. It answers the question: can a combination of variables be used to
predict group membership? Usually, several variables are included in a study to
see which ones contribute to the discrimination between groups.
\item Discriminant analysis is just the inverse of a one-way MANOVA, the multivariate analysis of variance. The levels of the independent variable (or factor) for Manova become the categories of the dependent variable for discriminant analysis, and the dependent variables of the Manova become the predictors for discriminant analysis. In MANOVA we ask whether group membership produces reliable differences on a combination of dependent variables. If the answer to that question is 'yes' then clearly that combination of variables can be used to predict group membership. 
\end{itemize}
%===========================================================================%
\subsection{Discriminant analysis}
\begin{itemize}

\item The major purpose of discriminant analysis is to predict membership in two or more mutually exclusive groups from a set of predictors, when there is no natural ordering on the groups. So we may ask whether we can predict whether people vote Labour or Conservative from a knowledge of their age, their class, attitudes, values etc etc.



\item Mathematically, MANOVA and discriminant analysis are the same; indeed, the SPSS MANOVA command can be used to print out the discriminant functions that are at the heart of discriminant analysis, though this is not usually the easiest way of obtaining them. 
\item These discriminant functions are the linear combinations of the standardised independent variables which yield the biggest mean differences between the groups. If the dependent variable is a dichotomy, there is one discriminant function; if there are k levels of the dependent variable, up to k-1 discriminant functions can be extracted, and we can test how many it is worth extracting. Successive discriminant functions are orthogonal to one another, like principal components, but they are not the same as the principal components you would obtain if you just did a principal components analysis on the independent variables, because they are constructed to maximise the differences between the values of the dependent variable.

\item The commonest use of discriminant analysis is where there are just two categories in the dependent variable; but as we have seen, it can be used for multi-way categories (just as MANOVA can be used to test the significance of differences between several groups, not just two). This is an advantage over logistic regression, which is always described for the problem of a dichotomous dependent variable.

\item You will encounter discriminant analysis fairly often in journals. But it is now being replaced with logistic regression, as this approach requires fewer assumptions in theory, is more statistically robust in practice, and is easier to use and understand than discriminant analysis. So we will concentrate on logistic regression.
\end{itemize}
%----------------------------------------------------------------------------------------------------------------------------------%



%---------------------------------------------------------------------------------------%
\subsection{Steps in Discriminant Analysis}
Discriminant function analysis is broken into a 2-step process: (1) testing
significance of a set of discriminant functions, and; (2) classification. The first
step is computationally identical to MANOVA. There is a matrix of total variances
and covariances; likewise, there is a matrix of pooled within-group variances and
covariances. The two matrices are compared via multivariate F tests in order to
determine whether or not there are any significant differences (with regard to all
variables) between groups. One first performs the multivariate test, and, if
statistically significant, proceeds to see which of the variables have significantly
different means across the groups.

%----------------------------------------------------------------------------------------------------------------------------------%
\begin{itemize}
\item Once group means are found to be statistically significant, classification of
variables is undertaken. DA automatically determines some optimal combination
of variables so that the first function provides the most overall discrimination
between groups, the second provides second most, and so on. Moreover, the
functions will be independent or orthogonal, that is, their contributions to the
discrimination between groups will not overlap. 
\item The first function picks up the
most variation; the second function picks up the greatest part of the unexplained
variation, etc... 
\item Computationally, a canonical correlation analysis is performed
that will determine the successive functions and canonical roots. Classification is
then possible from the canonical functions. Subjects are classified in the groups
in which they had the highest classification scores. 
\item The maximum number of
discriminant functions will be equal to the degrees of freedom, or the number of
variables in the analysis, whichever is smaller.
\end{itemize}
%---------------------------------------------------------------------------------------%
\subsubsection{Standardized coefficients and the structure matrix}
Discriminant functions are interpreted by means of standardized coefficients and
the structure matrix. Standardized beta coefficients are given for each variable in
each discriminant (canonical) function, and the larger the standardized
coefficient, the greater is the contribution of the respective variable to the
discrimination between groups. However, these coefficients do not tell us
between which of the groups the respective functions discriminate. We can
identify the nature of the discrimination for each discriminant function by looking
at the means for the functions across groups. Group means are centroids.
Differences in location of centroids show dimensions along which groups differ.
We can, thus, visualize how the two functions discriminate between groups by
plotting the individual scores for the two discriminant functions.
Another way to determine which variables define a particular discriminant
function is to look at the factor structure. The factor structure coefficients are the
correlations between the variables in the model and the discriminant functions.
The discriminant function coefficients denote the unique contribution of each
variable to the discriminant function, while the structure coefficients denote the
simple correlations between the variables and the functions.
%---------------------------------------------------------------------------%
\subsubsection{Summary}
To summarize, when interpreting multiple discriminant functions, which arise
from analyses with more than two groups and more than one continuous
variable, the different functions are first tested for statistical significance. If the
functions are statistically significant, then the groups can be distinguished based
on predictor variables. Standardized b coefficients for each variable are
determined for each significant function. The larger the standardized b
coefficient, the larger is the respective variable's unique contribution to the
discrimination specified by the respective discriminant function. In order to
identify which independent variables help cause the discrimination between
dependent variables, one can also examine the factor structure matrix with the
correlations between the variables and the discriminant functions. Finally, the
means for the significant discriminant functions are examined in order to
determine between which groups the respective functions seem to discriminate.



It is similar to regression analysis

We use maximum likelihood technique to assign a case to a group from a specified cut-off score.
\begin{itemize}
\itemIf group size is equal, the cut-off is mean score.
\itemIf group size is not equal, the cut-off is calculated from weighted means.
\end{itemize}

Grouping variables
\begin{itemize}
\item Categorical variables
\item Can have more than two values
\item The codes for the grouping variables must be integers
\item Independent variables
\item Continuous
\item Nominal variables must be recoded to dummy variables
\end{itemize}

%---------------------------------------------------------------------------------------%
\subsection{Discriminant function}
\begin{itemize}
\item A latent variable of a linear combination of independent variables
\item One discriminant function for 2-group discriminant analysis
\item For higher order discriminant analysis, the number of discriminant function is equal to g-1 (g is the number of categories of dependent/grouping variable).
\item The first function maximizes the difference between the values of the dependent variable.
\item The second function maximizes the difference between the values of the dependent variable while controlling the first function.
\item And so on.
\end{itemize}
The first function will be the most powerful differentiating dimension.
\item The second and later functions may also represent additional significant dimensions of differentiation

%---------------------------------------------------------------------------------------%
\subsection{Assumptions }
\begin{itemize}
\item Cases should be independent.
\item Predictor variables should have a multivariate normal distribution, and within-group variance-covariance matrices should be equal across groups.
\item Group membership is assumed to be mutually exclusive
\item The procedure is most effective when group membership is a truly categorical variable; if group membership is based on 
values of a continuous variable (for example, high IQ versus low IQ), consider using linear regression to take advantage of the richer 
information that is offered by the continuous variable itself.
\end{itemize}

Assumptions(similar to those for linear regression)
\begin{itemize}
\item Linearity, normality, multilinearity, equal variances
\item Predictor variables should have a multivariate normal distribution.
\item fairly robust to violations of the most of these assumptions. But highly sensitive to outliers.
\item Model specification
\end{itemize}

%---------------------------------------------------------------------------------------%
\subsection{Test of significance}
\begin{itemize}
\item For two groups, the null hypothesis is that the means of the two groups on the discriminant function-the centroids, are equal.
\item Centroids are the mean discriminant score for each group.
\item Wilk’s lambda is used to test for significant differences between groups.
\item Wilk’s lambda is between 0 and 1. It tells us the variance of dependent variable that is not explained by the discriminant function.
\item Wilk’s lambda is also used to test for significant differences between the groups on the individual predictor variables.
\item It tells which variables contribute a significant amount of prediction to help separate the groups
\end{itemize}


%---------------------------------------------------------------------------------------%
\subsection{Discriminant Analysis : Comparison to logistic regression}

Discriminant function analysis is very similar to logistic regression, and both can be used to answer the same research questions.[2] Logistic regression does not have as many assumptions and restrictions as discriminant analysis. However, when discriminant analysis’ assumptions are met, it is more powerful than logistic regression.[citation needed] Unlike logistic regression, discriminant analysis can be used with small sample sizes. It has been shown that when sample sizes are equal, and homogeneity of variance/covariance holds, discriminant analysis is more accurate.[3] With all this being considered, logistic regression is 
the common choice nowadays, since the assumptions of discriminant analysis are rarely met.

\end{document}
