\section{bigdata university}
\section{MariaDB}

\section{Education at 10gen}
education.10gen.com
This course will go over basic installation, JSON, schema design, querying, insertion of data, indexing and working with language drivers. We will also cover working in sharded and replicated environments. In the course, you will build a blogging platform, backed by MongoDB. Our code examples will be given in Python. A brief Python introduction is included in the course. 


Several worldwide reports have highlighted a shortage of talent in this area. 
The McKinsey Global Institute (2011) has reported that there will be a shortage 
of talent necessary for organisations to take advantage of business intelligence 
and data analytics. MGI predicts that “by 2018, the United States 
alone could face a shortage of 140,000 to 190,000 people with deep 
analytical skills as well as 1.5 million managers and analysts with
 the know-how to use the analysis of big data to make effective decisions”. 
A 2011 Bloomberg Businessweek Research Services survey of global businesses 
(published in 2012) shows that 97% of companies with revenues of more than 
$100 million are using some form of business analytics. 
43\% of these companies reported plans to increase analytical talent. Locally some estimates indicate a skills shortage of over 3,000 graduates with analytics skills in Ireland during the same period. This programme has been developed in consultation with large organisations in IT, finance and consulting in order to suit their business needs and thus securing future employment for graduates. 
%-----------------------------%
\section{Mortar Data}

Mortar Data is Hadoop for developers, plain and simple. The company has offered its cloud service — which replaces MapReduce with a combination Pig and Python — for almost a year. In November, it released the open source Mortar framework in order to build a community around sharing datasets and making it easier to write Hadoop pipelines. Mortar Data runs atop Amazon Web Services and currently supports Amazon S3 and MongoDB (hosted on Amazon EC2) as data sources.

%------------------------------%
Meter data management systems (MDMS):
Analytics-as-a-Service offerings
Advanced analytics as COTS software

\newpage

%=========================================================================================================%

Data Mining Notepad
Business intelligence (BI)
Concepts of data mining
Classification
Cluster analysis
CRISP-DM
Data Mining Concepts
Dredging
Extract, transform and load (ETL)
KDD : Knowledge Discovery in Databases.
KNIME
On-Line Analytic Processing (OLAP)
Predictive Model Markup Language
Predictive analytics
Quality and Integration
RapidMiner
Spatial data mining
Statistical data mining

Edit
Business intelligence (BI) 

Business intelligence (BI) is a broad category of applications and technologies for gathering, storing, analyzing, and providing access to data to help enterprise users make better business decisions. BI applications include the activities of decision support systems, query and reporting, online analytical processing (OLAP), statistical analysis, forecasting, and data mining.

Business intelligence applications can be:

Mission-critical and integral to an enterprise's operations or occasional to meet a special requirement
Enterprise-wide or local to one division, department, or project
Centrally initiated or driven by user demand

This term was used as early as September, 1996, when a Gartner Group report said:

By 2000, Information Democracy will emerge in forward-thinking enterprises, with Business Intelligence information and applications available broadly to employees, consultants, customers, suppliers, and the public. The key to thriving in a competitive marketplace is staying ahead of the competition. Making sound business decisions based on accurate and current information takes more than intuition. Data analysis, reporting, and query tools can help business users wade through a sea of data to synthesize valuable information from it - today these tools collectively fall into a category called "Business Intelligence."
Concepts of data mining

The most important data mining concepts are used for the analysis of collected information, most notably in the effort to observe a behavior. Unknown interactions between data are researched in a variety of ways to ascertain critical relationships between subjects and aggregated information. One challenge in data mining is that the actual information collected may not be reminiscent of the whole domain. In an effort to address this fact, correlations between the data can be methodically controlled by the various data mining concepts.

Standards for data mining concepts are enforced by the Association for Computing Machinery's Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD). This organization publishes the “International Journal of Information Technology and Decision Making” as well as the journal SIGKDD Explorations. Enforcing ethics and basic principles of data mining keeps the industry working efficiently and with limited legal problems.

Pre-processing of the information is one of the most important aspects of data mining. The raw data must be mined and interpreted. In order to perform this action, a process must be determined, the target data should be assembled and patterns are found. The process is known as Knowledge Discovery in Databases and was developed by Gregory Piatetsky-Shapiro in 1989.

Four different classes of data mining concepts allow the process to take place. Clustering uses the algorithm created from the data mining process to assemble items into similar groups. Unlike clustering, classification of the information is when the data is assembled into predefined groups and analyzed. Association attempts to find relationships between variables, determining which groups of data are commonly associated. The final type of data mining is regression, based on the method of identifying a function within the data collection.

Validating the information is the final step in discovering what the data mining application represents. When not all algorithms present a valid data set, the patterns that occur can result in a situation called overfitting. To overcome this problem, the data is compared to a test set. This is a concept in which the measurements are aligned with a series of algorithms that would provide a plausible set of data sets. If the acquired information does not line up to the test set, then the assumed patterns in the data must be inaccurate.

Some of the most important data mining concepts occur in a variety of industries. Gaming, business, marketing, science, engineering and surveillance all utilize data mining techniques. By conducting these techniques, each field can determine best practices or better ways to find results.

CRISP-DM
CRISP-DM stands for CRoss Industry Standard Process for Data Mining[. It is a data mining process model that describes commonly used approaches that expert data miners use to tackle problems. Polls conducted in 2002, 2004, and 2007 show that it is the leading methodology used by data miners

CRISP-DM breaks the process of data mining into six major phases::
Business Understanding
Data Understanding
Data Preparation
Modeling
Evaluation
Deployment

Business Understanding
This initial phase focuses on understanding the project objectives and requirements from a business perspective, and then converting this knowledge into a data mining problem definition, and a preliminary plan designed to achieve the objectives.

Data Understanding
The data understanding phase starts with an initial data collection and proceeds with activities in order to get familiar with the data, to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information.

Data Preparation
The data preparation phase covers all activities to construct the final dataset (data that will be fed into the modeling tool(s)) from the initial raw data. Data preparation tasks are likely to be performed multiple times, and not in any prescribed order. Tasks include table, record, and attribute selection as well as transformation and cleaning of data for modeling tools.

Modeling
In this phase, various modeling techniques are selected and applied, and their parameters are calibrated to optimal values. Typically, there are several techniques for the same data mining problem type. Some techniques have specific requirements on the form of data. Therefore, stepping back to the data preparation phase is often needed.

Evaluation
At this stage in the project you have built a model (or models) that appears to have high quality, from a data analysis perspective. Before proceeding to final deployment of the model, it is important to more thoroughly evaluate the model, and review the steps executed to construct the model, to be certain it properly achieves the business objectives. A key objective is to determine if there is some important business issue that has not been sufficiently considered. At the end of this phase, a decision on the use of the data mining results should be reached.

Deployment
Creation of the model is generally not the end of the project. Even if the purpose of the model is to increase knowledge of the data, the knowledge gained will need to be organized and presented in a way that the customer can use it. Depending on the requirements, the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data mining process. In many cases it will be the customer, not the data analyst, who will carry out the deployment steps. However, even if the analyst will not carry out the deployment effort it is important for the customer to understand up front what actions will need to be carried out in order to actually make use of the created models.


Data Mining Concepts
The process is known as Knowledge Discovery in Databases and was developed by Gregory Piatetsky-Shapiro in 1989. Four different classes of data mining concepts allow the process to take place. Clustering uses the algorithm created from the data mining process to assemble items into similar groups.



KDD : Knowledge Discovery in Databases.
KDD refers to the overall process of gleaning knowledge from data.

Knowledge discovery in databases (KDD), also referred to as (statistical) data mining, is an area of common interest to researchers in machine discovery, statistics, databases, knowledge acquisition, machine learning, data visualization, high performance computing, and knowledge-based systems.

Data Mining term is used interchangeably with KDD. In reality, it is one of the steps in the whole process of knowledge discovery in databases.

Data Mining needs a well defined business case and a diligent data preparation and has to be followed with a detailed evaluation of the discovery results.

KDD refers to the overall process of discovering useful knowledge from data, and data mining refers to a particular step in this process. Data mining is the application of specific algorithms for extracting patterns from data.

At an abstract level, the KDD field is concerned with the development of methods and techniques for making sense of data. The basic problem addressed by the KDD process is one of mapping low-level data (which are typically too voluminous to understand and digest easily) into other forms that might be more compact (for example, a short report), more abstract (for example, a descriptive approximation or model of the process that generated the data), or more useful (for example, a predictive model for estimating the value of future cases).

KDD is an attempt to address a problem that the digital information era made a fact of life for all of us: data overload.


On-Line Analytic Processing (OLAP)

The term On-Line Analytic Processing - OLAP (or Fast Analysis of Shared Multidimensional Information - FASMI) refers to technology that allows users of multidimensional databases to generate on-line descriptive or comparative summaries ("views") of data and other analytic queries. Note that despite its name, analyses referred to as OLAP do not need to be performed truly "on-line" (or in real-time); the term applies to analyses of multidimensional databases (that may, obviously, contain dynamically updated information) through efficient "multidimensional" queries that reference various types of data. 

OLAP facilities can be integrated into corporate (enterprise-wide) database systems and they allow analysts and managers to monitor the performance of the business (e.g., such as various aspects of the manufacturing process or numbers and types of completed transactions at different locations) or the market. The final result of OLAP techniques can be very simple (e.g., frequency tables, descriptive statistics, simple cross-tabulations) or more complex (e.g., they may involve seasonal adjustments, removal of outliers, and other forms of cleaning the data). 

Although Data Mining techniques can operate on any kind of unprocessed or even unstructured information, they can also be applied to the data views and summaries generated by OLAP to provide more in-depth and often more multidimensional knowledge. In this sense, Data Mining techniques could be considered to represent either a different analytic approach (serving different purposes than OLAP) or as an analytic extension of OLAP.
Predictive Model Markup Language 

The Predictive Model Markup Language (PMML) is an XML-based markup language developed by the Data Mining Group (DMG) to provide a way for applications to define models related to predictive analytics and data mining and to share those models between PMML-compliant applications.

PMML provides applications a vendor-independent method of defining models so that proprietary issues and incompatibilities are no longer a barrier to the exchange of models between applications. It allows users to develop models within one vendor's application and use other vendors' applications to visualize, analyze, evaluate or otherwise use the models. Previously, this was very difficult, but with PMML, the exchange of models between compliant applications is straightforward.
Since PMML is an XML-based standard, the specification comes in the form of an XML schema.
Predictive analytics 

Predictive analytics is the branch of data mining concerned with the prediction of future probabilities and trends. The central element of predictive analytics is the predictor, a variable that can be measured for an individual or other entity to predict future behavior. For example, an insurance company is likely to take into account potential driving safety predictors such as age, gender, and driving record when issuing car insurance policies.

Multiple predictors are combined into a predictive model, which, when subjected to analysis, can be used to forecast future probabilities with an acceptable level of reliability. In predictive modeling, data is collected, a statistical model is formulated, predictions are made and the model is validated (or revised) as additional data becomes available. Predictive analytics are applied to many research areas, including meteorology, security, genetics, economics, and marketing.

In a data warehouse, dirty data is a database record that contains errors. Dirty data can be caused by a number of factors including duplicate records, incomplete or outdated data, and the improper parsing of record fields from disparate systems. The Data Warehousing Institute (TDWI) estimates that dirty data costs U.S. businesses more than $600 billion each year.


RapidMiner
The Community Edition of RapidMiner (formerly "Yale") is an open source toolkit for data mining. Its strengths reside in part in its ability to easily define analytical steps (especially when compared with R), and in generating graphs more easily than e.g., R, or more effectively than MS Excel. 


What is it for?
RapidMiner is well suited for analyzing data generated by high-throughput instruments, e.g., genotyping, proteomics, and mass spectrometry. 

Example applications:
Bypassing its data mining functions and simply having RapidMiner generate figures.
Exploring your data in "super-Excel" fashion ("knowledge discovery").
Constructing custom data analysis workflows.
Calling RapidMiner functions from programs written in other languages/systems (e.g., Perl).
Notable selected features of RapidMiner:
Broad collection of data mining algorithms such as decision trees and self-organization maps.
Sophisticated graphics, such as overlapping histograms, tree charts and 3D scatter plots.
Many varied plugins, such as a text plugin for doing text analysis.

Spatial data mining 
Spatial data mining is the process of trying to find patterns in geographic data. Most commonly used in retail, it has grown out of the field of data mining, which initially focused on finding patterns in textual and numerical electronic information. Spatial data mining is considered a more complicated challenge than traditional mining because of the difficulties associated with analyzing objects with concrete existences in space and time.

As with standard data mining, spatial data mining is used primarily in the world of marketing and retail. It is a technique for making decisions about where to open what kind of store. It can help inform these decisions by processing pre-existing data about what factors motivate consumers to go to one place and not another.

Say that Ashley wants to open a nightclub on a certain city block. If she had access to the appropriate data, she could use spatial data mining to find out what spatial factors make night clubs successful. She might ask questions like: Will more people come to the club if public transit is nearby? What distance from other nightlife venues maximizes patronage? Is proximity to gas stations a plus or a minus?

Ashley might also want to ensure that the people who come to her nightclub arrive in an even distribution over the course of an individual night. She could also use spatial data mining—perhaps more accurately, spatiotemporal data mining—to find out how people move through the city at certain times. The same process could be applied to patronage on different nights of the week.

The difficulties of spatial data mining are a result of the complexity of the world beyond the internet. Whereas past efforts at data mining usually had databases ripe for analysis, the inputs available for spatial data mining are not grids of information but maps. These maps have different types of objects like roads, populations, businesses, and so on.
Determining whether something is "close to" something else goes from being a discrete to a continuous variable. This massively increases the complexity required for analysis. Incredibly, this is one of the more simple types of relationships available to someone attempting spatial data mining.

Spatial data mining also faces the problem of false positives. In the process of searching data looking for relationships, many apparent trends will emerge as a consequence of statistical false positives. This problem also exists for the task of mining a more simple database, but it is amplified by the magnitude of data available to the spatial data miner. 

Ultimately, a trend identified by spatial data mining should be confirmed through the process of explanation and additional research.

