<span style="text-decoration:underline;"><strong>Statistical Learning</strong></span>
<ul>
	<li>StatLearning Course Website - <a title="statlearning" href="https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about" target="_blank">Here</a></li>
	<li>Website for Recommended Text - <em>An Introduction to Statistical Learning with Applications in R</em> <a title="ISL book" href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank">Here</a></li>
</ul>
<span style="text-decoration:underline;"><strong>Syllabus </strong></span>

The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines.

Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).

<span style="text-decoration:underline;"><strong>Part 1 Introduction</strong></span>

1.1 Opening Remarks
1.2 Examples and Framework
<ul>
	<li>Introduction to R (<a href="https://dl.dropboxusercontent.com/u/6044937/ISL%20Data%20Science/ISL-ch2r.html" target="_blank">ch2.R</a> - Implemented with R Markdown)   (<em>Auto</em> replaced with <em>mtcars</em>)</li>
</ul>
<span style="text-decoration:underline;"><strong>Part 2 Statistical Learning</strong></span>

2.1 Introduction to Regression Models
2.2 Dimensionality and Structured Models
2.3 Model Selection and Bias-Variance Tradeoff
2.4 Classification - <a href="https://dl.dropboxusercontent.com/u/6044937/ISL%20Data%20Science/ISL2-Classification.html" target="_blank">Notes</a>

<span style="text-decoration:underline;"><strong>Part 3 Linear Regression</strong></span>
<ul>
	<li>Linear Regression R code - <a href="https://dl.dropboxusercontent.com/u/6044937/ISL%20Data%20Science/ch3.R" target="_blank">Here</a></li>
</ul>
<span style="text-decoration:underline;"><strong style="line-height:1.714285714;font-size:1rem;">Part 4 Classification</strong></span>

<span style="text-decoration:underline;"><strong>Part 5 Resampling</strong></span>

The bootstrap allows us to mimic the process of  sampling new datasets. It allows us, for example, to assess the sampling variance of an estimator without making strong theoretical assumptions.

Cross-validation cleverly divides a single dataset multiple times into training and test sets, to help us select meaningfully from a collection of models.

<span style="text-decoration:underline;"><strong>Part 6 Model Selection</strong></span>
<ul>
	<li>The Akaike Information Criterion (<a href="http://en.wikipedia.org/wiki/Akaike_information_criterion" target="_blank">Wikipedia</a>)</li>
</ul>
<span style="text-decoration:underline;"><strong>Part 7 Nonlinear</strong></span>

<span style="text-decoration:underline;"><strong>Part 8 Trees</strong></span>

<span style="text-decoration:underline;"><strong>Part 9 </strong><strong>Support Vector Machines </strong></span>
<div><span style="font-size:1rem;line-height:1;">Support Vector Machines are very specific class of algorithms, characterized by usage of kernels, absence of local minima, sparseness of the solution and capacity control obtained by acting on the margin, or on number of support vectors, etc.</span></div>
<div>
<ul>
	<li> <span style="line-height:1.714285714;font-size:1rem;">An Idiot’s guide to Support vector machines (SVMs) (</span><a style="line-height:1.714285714;font-size:1rem;" title="IdiotsGuideToSVMs" href="http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf" target="_blank">Here</a><span style="line-height:1.714285714;font-size:1rem;">)</span></li>
</ul>
</div>
<span style="text-decoration:underline;"><strong>Part 10 Unsupervised</strong></span>
<div></div>
<div></div>
