\documentclass[a4]{beamer}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{newlfont}
\usepackage{multicol}
\usepackage{amsmath,amsthm,amsfonts}
%\usepackage{beamerthemesplit}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{mathptmx} % Font Family
\usepackage{helvet} % Font Family
\usepackage{color}
\mode<presentation> {
\usetheme{Default} % was Frankfurt
\useinnertheme{rounded}
\useoutertheme{infolines}
\usefonttheme{serif}
%\usecolortheme{wolverine}
% \usecolortheme{rose}
\usefonttheme{structurebold}
}
\setbeamercovered{dynamic}
\title[MA4128]{Advanced Data Modelling \\ {\normalsize MA4128 2016 Week 2}}
\author[Kevin O'Brien]{Kevin O'Brien \\ {\scriptsize Kevin.obrien@ul.ie}}
\date{Spring Semester 2016}
\institute[Maths \& Stats]{Dept. of Mathematics \& Statistics, \\ University \textit{of} Limerick}

\renewcommand{\arraystretch}{1.5}
%------------------------------------------------%
\begin{document}

\begin{frame}
\titlepage
\end{frame}

%--------------------------------------------------------------------------------------------------------------------------%
\begin{frame}
	\frametitle{Hypothesis Testing}
	\large
	Recall from Hypothesis Testing: the inferential step to conclude that the null hypothesis is false goes as follows:\\  The data (or data more extreme) are very unlikely given that the null hypothesis is true.
	\bigskip
	This means that:
	\begin{itemize}\item [(1)] a very unlikely event occurred or
		\item[(2)] the null hypothesis is false. \end{itemize} \smallskip
	The inference usually made is that the null hypothesis is false. Importantly it doesnt prove the null hypothesis to be false.
\end{frame}
%-------------------------------------------------------------------------------------------------------------------------%
\begin{frame}
	\frametitle{Type I and II errors}
	\large
	There are two kinds of errors that can be made in hypothesis testing:
	\begin{itemize}
		\item[(1)] a true null hypothesis can be incorrectly rejected
		\item[(2)] a false null hypothesis can fail to be rejected.
	\end{itemize}\smallskip
	The former error is called a \textbf{\emph{Type I error}} and the latter error is called a \textbf{\emph{Type II error}}. \\ \bigskip
	The probability of Type I error is always equal to the level of significance $\alpha$ (alpha) that is used as the standard for rejecting the null hypothesis .
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
	\frametitle{Type II Error}
	\large
	\begin{itemize}
		
		\item The probability of a Type II error is designated by the Greek letter beta ($\beta$). \smallskip
		\item A Type II error is only an error in the sense that an opportunity to reject the null hypothesis correctly was lost. \smallskip
		\item It is not an error in the sense that an incorrect conclusion was drawn since no conclusion is drawn when the null hypothesis is not rejected.
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Hypothesi Testing}
	\large
	\noindent \textbf{Statistical Power}
	\begin{itemize}
\item 	The power of any test of statistical significance is defined as the probability that it will reject a false null hypothesis. \item Statistical power is inversely related to beta or the probability of making a Type II error.
\item  In short, power $= 1 - \beta$.
	\end{itemize}

\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
	\frametitle{Types of Error}
	\large
	\begin{itemize}
		\item
		A Type I error, on the other hand, is an error in every sense of the word. A conclusion is drawn that the null hypothesis is false when, in fact, it is true. \item Therefore, Type I errors are generally considered more serious than Type II errors. \smallskip
		\item
		The probability of a Type I error ($\alpha$ ) is set by the experimenter. \smallskip \item There is a trade-off between Type I and Type II errors. The more an experimenter protects himself or herself against Type I errors by choosing a low level, the greater the chance of a Type II error.
	\end{itemize}
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
	\frametitle{Types of Error}
	\large
	\begin{itemize}
		\item
		Requiring very strong evidence to reject the null hypothesis makes it very unlikely that a true null hypothesis will be rejected. \smallskip \item However, it increases the chance that a false null hypothesis will not be rejected, thus increasing the likelihood of Type II error.\smallskip
		\item
		The Type I error rate is almost always set at 0.05 or at 0.01, the latter being more conservative since it requires stronger evidence to reject the null hypothesis at the 0.01 level then at the 0.05 level. \smallskip
		\item \textbf{Important}In this module, the significance level $\alpha$ can be assumed to be 0.05, unless explicitly stated otherwise.
	\end{itemize}
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
	\frametitle{Type I and II errors}
	\large
	These two types of errors are defined in the table below.
\Large
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			& True State:  & True State: \\
			& H0 True & H0 False \\
			
			\hline
			Decision: Reject H0 & Type I error& Correct\\\hline
			Decision: Do not Reject H0 & Correct &Type II error\\ \hline
		\end{tabular}
	\end{center}
\end{frame}
\begin{frame}
	\frametitle{Binary Classification}
	\large
	\begin{itemize}
		\item 
	Statistical classification in general is one of the problems studied in computer science, in order to automatically learn classification systems. \smallskip
	\item Some methods suitable for learning binary classifiers include the decision trees, Bayesian networks, support vector machines, neural networks, probit regression, and logit regression.
	\end{itemize}
\end{frame}
%=========================================================== %
\begin{frame}
	\frametitle{Binary Classification}
	\large
\begin{itemize}
\item 	\textbf{Binary} or \textbf{Binomial classification} is the task of classifying the elements of a given set into two groups on the basis of a classification rule.
\item Algorithms for Binary Classification Include:
\begin{enumerate}
	\item Logistic Regression
	\item Support Vector Machines
\end{enumerate}
\end{itemize}
\end{frame}
%----------------------------------------------------------- %
\begin{frame}
	\frametitle{Binary Classification}
	\large
. Some typical binary classification tasks are:
\begin{itemize}
\item	medical testing to determine if a patient has certain disease or not – the classification property is the presence of the disease;
\item	A "\textbf{pass or fail}" test method or quality control in factories; i.e. deciding if a specification has or has not been met: a Go/no go classification.
\item 	An item may have a qualitative property; it does or does not have a specified characteristic
\item	information retrieval, namely deciding whether a page or an article should be in the result set of a search or not – the classification property is the relevance of the article, or the usefulness to the user.
\end{itemize}	


\end{frame}
%--------------------------------%

\begin{frame}
	\frametitle{Medical testing example}
	\begin{itemize}
		\item True positive = Sick people correctly diagnosed as sick
		
		\item False positive= Healthy people incorrectly identified as sick
		
		\item True negative = Healthy people correctly identified as healthy
		
		\item False negative = Sick people incorrectly identified as healthy.
	\end{itemize}
\end{frame}
\subsection*{Definitions}
\begin{frame}
	\textbf{Accuracy Rate}\\
	The accuracy rate calculates the proportion ofobservations being allocated to the \textbf{correct} group by the predictive model. It is calculated as follows:
	\[ \frac{
		\mbox{Number of Correct Classifications }}{\mbox{Total Number of Classifications }} \]
	
	\[ = \frac{TP + TN}{TP+FP+TN+FN}\]
	
\end{frame}
\begin{frame}
	\noindent \textbf{Misclassification Rate}\\
	The misclassification rate calculates the proportion ofobservations being allocated to the \textbf{incorrect} group by the predictive model. It is calculated as follows:
	\[ \frac{
		\mbox{Number of Incorrect Classifications }}{\mbox{Total Number of Classifications }} \]
	
	\[ = \frac{FP + FN}{TP+FP+TN+FN}\]
\end{frame}
%==================================================================== %
\begin{frame}	\frametitle{Binary Classification}
	\large
Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as classification function:

\begin{itemize}
\item \textbf{Sensitivity} (also called the true positive rate, or the recall in some fields) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).
\item \textbf{Specificity} (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).
Thus sensitivity quantifies the avoiding of false negatives, as specificity does for false positives
\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Binary Classification}
	\large
	Recall the possible outcomes of a hypothesis test procedure. In particular recall the two important types of error. Importantly the binary classification prediction procedure can yield wrong predictions.
	\begin{center}
		\begin{tabular}{|c|c|c|} \hline
			& Null hypothesis & Null hypothesis   \\
			& ($H_0$) true	 & ($H_0$) false \\ \hline
			Reject 	       & \textbf{Type I error }   & \textbf{Correct outcome} \\
			null hypothesis& False positive  & True positive \\ \hline
			Fail to reject & \textbf{Correct outcome} & \textbf{Type II error} \\
			null hypothesis & True negative  & False negative \\ \hline
		\end{tabular} 
	\end{center}
\end{frame}
\begin{frame}
	\frametitle{Accuracy, Precision and Recall}
	\large
	\noindent \textbf{Confusion Matrix}
	\begin{itemize}
	\item A confusion matrix is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. \item This allows more detailed analysis than mere proportion of correct guesses (accuracy). \item Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the number of samples in different classes vary greatly).
	\end{itemize}

\end{frame}

%------------------------------------------------------%
\begin{frame}
	\frametitle{Accuracy, Precision and Recall}
	\large
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline  & Predicted Negative & Predicted Positive \\ 
			\hline Observed Negative & True Negative & False Positive \\
			& (TN) & (FP)  
			\\
			\hline Observed Positive & False Negative & True Positive \\ 
			& (FN) & (TP)  \\
			\hline 
		\end{tabular} 
	\end{center}
\textit{	(Notice that ``Negative" precedes ``Positive")}
\end{frame}
\begin{frame}
	\frametitle{Precision and Recall}
	\Large
	\begin{itemize}
		\item \textbf{Precision} is the number of correct positive results divided by the number of \textit{\textbf{predicted positive}} results.
		\[ \mbox{Precision}= \frac{TP}{TP+FP}  \]
		\item \textbf{Recall} is the number of correct positive results divided by the number of \textit{\textbf{actual positive}} results. 
		\[ \mbox{Recall}= \frac{TP}{TP+FN}  \]
	\end{itemize}
\end{frame}
%------------------------------------------------------%
\begin{frame}
	\frametitle{Accuracy, Precision and Recall}
	\large
	Important metrics for determining how usefulness of the prediction procedure are : \textbf{Accuracy}, \textbf{Recall} and \textbf{Precision}.
	\\ \bigskip
	Accuracy, Precision and Recall are defined as
	\[\mbox{Accuracy}=\frac{TP+TN}{TP+TN+FP+FN} \]
	\[\mbox{Precision}=\frac{TP}{TP+FP} \] 
	\[\mbox{Recall}=\frac{TP}{TP+FN} \]
\end{frame}
%------------------------------------------------------%
\begin{frame}
	\frametitle{Accuracy, Precision and Recall}
	Another measure is the F-measure (or F-score), which is computed as
	\[F = 2 \cdot \bigg( \frac{\mathrm{precision} \cdot \mathrm{recall}}{ \mathrm{precision} + \mathrm{recall}} \bigg)\]
	
\end{frame}
%------------------------------------------------------%
\begin{frame}
	\frametitle{Questions}
	\large
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			& Predicted  & Predicted \\
			& Negative & Positive \\ \hline
			Negative Cases & TN: 9,700  & FP: 165 \\ \hline
			Positive Cases & FN: 35 & TP: 100 \\ \hline
		\end{tabular} 
	\end{center}
\end{frame}
%------------------------------------------------------%
\begin{frame}
	\frametitle{Accuracy, Precision and Recall}
	
	With reference to the table on the previous slide, compute each of the following appraisal metrics.
	\begin{multicols}{2} 
		\begin{itemize}
			\item[a.] Accuracy
			\item[b.] Precision
			\item[c.] Recall
			\item[d.] $F$ measure
		\end{itemize}
	\end{multicols}
\end{frame}
%------------------------------------------------------%
\begin{frame}
	\frametitle{Accuracy, Precision and Recall}
	\begin{itemize}
		\item Why is the accuracy value so high?
		\item Why is the F-measure so low?
		\begin{itemize}
\item[$\ast$] This is the \textbf{class-imbalance} problem: more ``negative" outcomes which skews the statistic, but these outcomes are the least relevant.
\item[$\ast$] The F-measure disregards the irrelevant ``true negatives, and concentrates on the more relevant potential outcomes.

		\end{itemize}
		\end{itemize}
\end{frame}
%-------------------------------------------%
\begin{frame}
	\frametitle{The F-score}
	\Large
	\begin{itemize}
		\item The F-score or F-measure is a single measure of a classification procedure's usefulness. 
		\item The F-score considers both the \textit{\textbf{Precision}} and the \textit{\textbf{Recall}} of the procedure to compute the score.
		\item The higher the F-score, the better the predictive power of the 
		classification procedure. 
		\item A score of 1 means the classification procedure is perfect. The lowest possible F-score is 0.
		\[ 0 \leq F \leq 1 \]
	\end{itemize}
\end{frame}
%-------------------------------------------%

%-------------------------------------------%
\begin{frame}
	\frametitle{The F-score}
	\Large The F-score is the Harmonic mean of Precision and Recall.
	\[ F = \frac{2}{\frac{1}{\mbox{Recall}} + \frac{1}{\mbox{Precision}}} \]
	Alternatively
	\[ F = 2 \times \left( \frac{\mbox{Precision} \times \mbox{Recall}}{\mbox{Precision} + \mbox{Recall}} \right) \] 
\end{frame}
%-------------------------------------------%
\begin{frame}
	\frametitle{Class Imbalance}
	\large
	\noindent \textbf{Class Imbalance Problem}
	\begin{itemize}
		\item It is the problem in machine learning where the total number of one class of data (\textbf{positive}) is far less than the total number of another class of data (\textbf{negative}). 
		\item This problem is extremely common in practice and can be observed in various disciplines including fraud detection, anomaly detection, medical diagnosis, oil spillage detection, facial recognition, etc.
		\item Most machine learning algorithms and works best when the number of instances of each classes are roughly equal. 
		\item When the number of instances of one class far exceeds the other, problems arise (\textit{Type I  and Type II error}).
	\end{itemize}
	
\end{frame}


%--------------------------------%
\begin{frame}
	\Large
	Number of cases: \textbf{100,000}\\ 
%	\begin{center}
%		
%		\begin{table}[!htbp]
%			%\caption{Comparison of percentages.}
%			\begin{tabular}{c *4c}
%				\toprule
%				Actual &  \multicolumn{2}{c}{Predicted} & \multicolumn{2}{c}{Predicted}\\
%				State &  \multicolumn{2}{c}{Negative} & \multicolumn{2}{c}{Positive}\\
%				\midrule
%				Negative   & \phantom{spa} TN & \textbf{97750}\phantom{spa}   & FP  & \textbf{150}\\
%				Positive   & \phantom{spa} FN & \textbf{330} \phantom{spa}   & TP  & \textbf{1770}\\
%				\bottomrule
%			\end{tabular}
%		\end{table}
%	\end{center}
{
	\large
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline  & Predicted Negative & Predicted Positive \\ 
			\hline Observed Negative & True Negative & False Positive \\
			& 97750 & 150  
			\\
			\hline Observed Positive & False Negative & True Positive \\ 
			& 330 & 1770  \\
			\hline 
		\end{tabular} 
	\end{center}
}
	\begin{itemize}
		\item \textbf{Accuracy} = 0.9952
		\item \textbf{Recall} = 0.8428
		\item \textbf{Precision} = 0.9218
	\end{itemize}
\end{frame}
%--------------------------------%
\begin{frame}
	\frametitle{The F-score}
	\Large
	\[ F = 2 \times \frac{\mbox{Precision} \times \mbox{Recall}}{\mbox{Precision} + \mbox{Recall}}\]\bigskip
	\[ F = 2 \times \frac{\mbox{0.9218} \times \mbox{0.8428}}{\mbox{0.9218} + \mbox{0.8428}} = 2 \times \left( \frac{\mbox{0.9218} \times \mbox{0.8428}}{\mbox{0.9218} + \mbox{0.8428}} \right)\]



	
	\[ F = 2 \times \left( \frac{0.7770}{1.7646} \right) = 2 \times 0.4402 \]
		
		\[F = 0.8804\] 
	
\end{frame}



%---------------------------- %

\end{document}