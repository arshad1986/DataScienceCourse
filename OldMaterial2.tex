MA4125 Data Analysis - Lecture 5a



Overview
Data architecture
Web databases
Data Visualization
Visual data mining
Data warehousing
Data marts
Data mining vs data warehousing
The data mining process
Important data mining concepts
Data mining project
Business Intelligence
Statistical data mining
Business intelligence tools
What is Statistical Data Analysis?
Defining the Problem
Collecting the Data
Analyzing the Data
Reporting the Results
Data Analysis
Data Integration
Sources of Data
Data Collection
Data Validation
Data Cleansing
The process of data cleansing
The process of data analysis
Commercial Databases
Data dredging
Database and SQL
Structured Query Language (SQL)
Queries
Data definition
Data manipulation
Database administration
Data Leaks
Decision Support Systems
Database
Database Management System


Vast amounts of statistical information are available in today's global and economic environment because of continual improvements in computer technology. To compete successfully globally, managers and decision makers must be able to understand the information and use it effectively. Statistical data analysis provides hands on experience to promote the use of statistical thinking and techniques to apply in order to make educated decisions in the business world.

Computers play a very important role in statistical data analysis. The statistical software package, SPSS, which is used in this course, offers extensive data-handling capabilities and numerous statistical analysis routines that can analyze small to very large data statistics. The computer will assist in the summarization of data, but statistical data analysis focuses on the interpretation of the output to make inferences and predictions.

Studying a problem through the use of statistical data analysis usually involves four basic steps.

1. Defining the problem 
2. Collecting the data 
3. Analyzing the data 
4. Reporting the results

Defining the Problem

An exact definition of the problem is imperative in order to obtain accurate data about it. It is extremely difficult to gather data without a clear definition of the problem.

Collecting the Data

We live and work at a time when data collection and statistical computations have become easy almost to the point of triviality. Paradoxically, the design of data collection, never sufficiently emphasized in the statistical data analysis textbook, have been weakened by an apparent belief that extensive computation can make up for any deficiencies in the design of data collection. One must start with an emphasis on the importance of defining the population about which we are seeking to make inferences, all the requirements of sampling and experimental design must be met.

Designing ways to collect data is an important job in statistical data analysis. Two important aspects of a statistical study are: 
Population - a set of all the elements of interest in a study 
Sample - a subset of the population 

Statistical inference is refer to extending your knowledge obtain from a random sample from a population to the whole population. This is known in mathematics as an Inductive Reasoning. That is, knowledge of whole from a particular. Its main application is in hypotheses testing about a given population. 
The purpose of statistical inference is to obtain information about a population form information contained in a sample. It is just not feasible to test the entire population, so a sample is the only realistic way to obtain data because of the time and cost constraints. 

For the purpose of statistical data analysis, distinguishing between cross-sectional and time series data is important. Cross-sectional data re data collected at the same or approximately the same point in time. Time series data are data collected over several time periods.

Data can be collected from existing sources or obtained through observation and experimental studies designed to obtain new data. In an experimental study, the variable of interest is identified. Then one or more factors in the study are controlled so that data can be obtained about how the factors influence the variables. In observational studies, no attempt is made to control or influence the variables of interest. A survey is perhaps the most common type of observational study.



Reporting the Results

Through inferences, an estimate or test claims about the characteristics of a population can be obtained from a sample. The results may be reported in the form of a table, a graph or a set of percentages. Because only a small collection (sample) has been examined and not an entire population, the reported results must reflect the uncertainty through the use of probability statements and intervals of values.

To conclude, a critical aspect of managing any organization is planning for the future. Good judgment, intuition, and an awareness of the state of the economy may give a manager a rough idea or "feeling" of what is likely to happen in the future. However, converting that feeling into a number that can be used effectively is difficult. Statistical data analysis helps managers forecast and predict future aspects of a business operation. The most successful managers and decision makers are the ones who can understand the information and use it effectively.


%=================================================================================%
\section{Web databases}
A web database is a system for storing information that can then be accessed via a website. For example, an online community may have a database that stores the username, password, and other details of all its members. The most commonly used database system for the internet is MySQL due to its integration with PHP — one of the most widely used server side programming languages.

At its most simple level, a web database is a set of one or more tables that contain data. Each table has different fields for storing information of various types. These tables can then be linked together in order to manipulate data in useful or interesting ways. In many cases, a table will use a primary key, which must be unique for each entry and allows for unambiguous selection of data.

A web database can be used for a range of different purposes. Each field in a table has to have a defined data type. For example, numbers, strings, and dates can all be inserted into a web database. Proper database design involves choosing the correct data type for each field in order to reduce memory consumption and increase the speed of access. Although for small databases this often isn't so important, big web databases can grow to millions of entries and need to be well designed to work effectively.

Content management systems commonly use web databases to store information such as posts, usernames, and comments. Using a database allows the website to be updated easily and without the need to edit the HTML code for each individual page. Not only is this a much more efficient way of creating and updating a website, but it also makes the process more accessible to people who aren't fluent in the programming languages of the Internet.

An example of where a web database may be used is for an online forum. Forum software often creates a database with a number of tables, including one for users, posts, and settings. It is important for the relationships between the database tables to be properly set and defined so that posts and users can be linked together easily.

In some cases, web databases can be bought with information already included. For example, a database could include a list of all the dentists in the US along with state and address. These databases are commonly integrated into related websites using PHP and HTML, along with additional content.
\section{Data Visualization}
Data analysis is a process that requires the reviewing of data to determine trends and abnormalities for a business. This is a daunting task because large data sets are typically presented in a spread sheet format. Data visualization is the process of converting standard textual data into pictures that are easily understood by a general audience. These pictures typically include graphs, shapes, and abstract objects that are color coded based on the significances of the data.

A good example of data visualization in practice is in an automobile navigation system. These systems are widely used today as an intuitive method of providing directions for drivers. This software provides graphical road maps that explain in detail, step-by-step directions. These road maps are easier to understand than written directions because they provide a visual map that represents the coordinates.

Many businesses use data visualization as a tool to provide the status on critical projects. This is often referred to as a corporate status dashboard. The dashboard provides vital statistics on goals, expenses, and overall stability of the company. Having a dashboard assists senior managers by quickly providing graphics that indicate issues within the company. These graphics can be presented in a red-light, green-light format, which makes them easy to comprehend.

The scientific community has been using data visualization techniques for many decades to solve complex problems. This is often found in weather programs and seismic activity monitoring systems. Providing complex data elements in a graphical picture makes it easier for humans to understand the relevance and relationships of data.

The daily weather report is another example of data visualization in practice. Most television weather reports present weather with graphical representations of rain, wind, and snow. These maps are layered with the images of the global jet stream and temperatures to present a full view of the weather. Without visualization, this information would be difficult to understand.

Online traffic reporting uses data visualization techniques to indicate traffic congestion in most cities. This report is typically presented with graphical automobiles and uses colors to indicate average speed. The normal color scheme for this information is green, yellow, and red. A user of the traffic reporting system can quickly identify congestion problems by looking for roadways highlighted in the color red.

An air traffic control system is another example of data visualization in practice. This system is designed to show air traffic and congestion problems in the sky. The system notifies users of impending issues by changing the colors of the graphical airplanes. When a plane is in trouble, the screen color changes to red. This provides an emergency warning to the air traffic controller.
Visual data mining
In visual data mining, programmers build interfaces that allow for visual presentations to be a part of how human users interpret the data that they see. This kind of program is being built in various industries. Scientists and programmers are still looking at the possibilities of visual data mining, and suggesting some recommendations for best possible methods.

According to top theorists, simplicity is key for these kinds of interfaces. These programs must be able to display data in ways that make it easy for human users to “see” it. Those looking at involving visual data mining also see “leading” as a potential problem in visual data mining applications. Users need to be able to objectively draw their own conclusions from a visual data mining presentation.

Other issues in visual data mining involve whether the program is available to a large audience. Creators of these programs should also think about whether security is sufficient to protect the programs from unauthorized use. The overall design of systems must be precise. They should be easily accessible to users, secure from hackers, and made to be inherently powerful data presentations.

In some industries, visual data mining applications and similar programs are evolving at an amazing rate. Many who are engaged in these sectors find it useful to keep track of the progress that others are making in related technologies. Visual and interactive data mining is expected to be an important tool for the future of the interaction between humans and computers.

\section{Data warehousing}
Data warehousing has become increasingly important to modern companies as the amount of data necessary to remain competitive and make informed decisions about the future of a company has become greater and greater. A company's entire data warehouse often consists of a collection of data marts, sets of data that typically are focused on a particular department or area of the company. Combined into a data mart warehouse, these data marts help support the information system of the entire company.

\section{Data Marts}
A data mart might be dedicated to customer data or sales information, or it could be constructed specifically to support a company's human resources department. In some cases, data marts are compiled from data from the main data warehouse. Many companies, though, take a bottom-up approach to data warehousing and instead build the overall data warehouse by compiling data mart data into smaller systems, then combining them into a company-wide data warehouse.

One important factor in building a data mart is usability. Because the data mart usually is aimed at supplying information to a specific department for a specific purpose, the data must be easily accessed and processed. A company's Information Technology department or Management Information Systems specialists often focus on this area, ensuring that an easy-to-use interface exists so that employees, including managers and other high-level personnel, can access the data mart data and use it within the course of their jobs.

Individual data mart design depends on the specific needs of the company or the individual department. Most data is organized into relational databases, with a database management system implemented company-wide in order to let individual employees easily access data, generate reports or perform other necessary day-to-day functions. Some companies require more complex data processing that makes use of multidimensional databases. Still others use database functionality that allows multidimensional data analysis while still using the simpler and less costly relational database management system.

For the most part, data warehouses are built by adding data as it comes into the company's systems. Updates to data are not as important to an overall view of company performance as data that has been added to and compiled over a longer period of time. Managers can use data from the data mart to view company trends, sales patterns and efficiency of individual departments and to make forecasts about future performance. At this strategic planning level, the data mart and data warehouse are invaluable to those planning the future of the company.
\section{Data mining vs data warehousing} 
The terms data mining and data warehousing are often confused by both business and technical staff. The entire field of data management has experienced a phenomenal growth with the implementation of data collection software programs and the decreased cost of computer memory. The primary purpose behind both these functions is to provide the tools and methodologies to explore the patterns and meaning in large amount of data.

The primary differences between data mining and data warehousing are the system designs, methodology used, and the purpose. Data mining is the use of pattern recognition logic to identity trends within a sample data set and extrapolate this information against the larger data pool. Data warehousing is the process of extracting and storing data to allow easier reporting.

Data mining is a general term used to describe a range of business processes that derive patterns from data. Typically, a statistical analysis software package is used to identify specific patterns, based on the data set and queries generated by the end user. A typical use of data mining is to create targeted marketing programs, identify financial fraud, and to flag unusual patterns in behavior as part of a security review.

An excellent example of data mining is the process used by telephone companies to market products to existing customers. The telephone company uses data mining software to access its database of customer information. A query is written to identify customers who have subscribed to the basic phone package and the Internet service over a specific time frame. Once this data set is selected, another query is written to determine how many of these customers took advantage of free additional phone features during a trial promotion. The results of this data mining exercise reveal patterns of behavior that can drive or help refine a marketing plan to increase the use of additional telephone services.

It is important to note that the primary purpose of data mining is to spot patterns in the data. The specifications used to define the sample set has a huge impact on the relevance of the output and the accuracy of the analysis. Returning to the example above, if the data set is limited to customers within a specific geographical area, the results and patterns will differ from a broader data set. Although both data mining and data warehousing work with large volumes of information, the processes used are quite different.

A data warehouse is a software product that is used to store large volumes of data and run specifically designed queries and reports. Business intelligence is a growing field of study that focuses on data warehousing and related functionality. These tools are designed to extract data and store it in a method designed to provide enhanced system performance. Much of the terminology in data mining and data warehousing are the same, leading to more confusion.

\subsection{Data mining project}
A data mining project is typically initiated by the business managers or analysts. The purpose of data mining is to identify patterns or trends in large sets of data. For example, a data mining project into customer purchasing trends can help inform the decision making process surrounding the launch of a new product, customer preferences, and priorities.

Every data mining project requires a combination of staff from a range of different areas within the organization. The project manager is usually someone with expertise in business intelligence tools, data mining, and data warehouse support. The subject matter experts are recruited from the different practice areas within the organization. For example, staff from marketing, sales, and accounts receivable would all add value to the project. Information technology staff included in the project would include systems and business analysts.

There are two primary requirements to support a data mining project: software and skill set. There is a wide range of business intelligence software that provides the tools necessary to support data mining. In general, the transactional data needs to be identified and moved into the data warehouse. Once the relational database structure has been created, the data mining tools are used to create custom queries, data cubes, and reports.

Staff working on a data mining project require skills in statistics, information technology, data management, and data integrity issues. The software used in this type of project is very complex, and user training is necessary to utilize the functionality provided. For many organizations, the costs associated with a data mining project must be carefully measured against the possible benefits of this technology.

There are four stages to a data mining project: a requirements document, defining user specifications, implementing the database, and writing queries and reports. The requirements document is created by the project manager, based on discussions with the project sponsor. The purpose of this document is to clearly state the project scope, resources, time line, and delivery date. It is very common to require signatures from the project sponsor and executive level to ensure that senior management has approved the project.

User specifications are often created by a team of business analysts and end users. This process typically requires a series of meetings, review of documentation, and editing. The more collaborative the process, the more satisfactory the end results will be. Working together results in expanded understanding of the users' perspective by the business analysts.

The specifications and requirements document are provided to the business intelligence system team, who isresponsible for creating the database structure, extracting the required data, and working with other team members to set up the infrastructure necessary to support this initiative. This aspect of the project requires time, software, and hardware. In many organizations, a cost benefit analysis is submitted to the project sponsor at this stage, so that funding can be secured.

The actual queries and reports are based on user needs and must be tested by the business analysts before implementation. Look at the core business functions and current reports to see what information is required and determine if data mining is an appropriate tool to meet this need.

\section{Business intelligence tools}
Business intelligence tools are software programs and features that are used to complete detailed data analysis. The term "business intelligence" is used to describe computer software that is used to identify patterns and trends in large data sets. There are a range of business intelligence tools available in the commercial software market, and selecting the correct solution requires a good understanding of reporting requirements and long-term business needs.

There are three types of business intelligence tools: data extraction, data architecture and query development. All four types of tools are included as standard features in most business intelligence software products. The skills required to utilize these tools effectively include data management, intermediate computer skills and prior statistics training. Business intelligence is an area that has experienced growth and and is expected to keep growing.

Business intelligence systems are either built into an enterprise resource planning (ERP) system or function as independent computer applications. In order to utilize business intelligence software, the data from the transactional system must be made available to the software. This usually is achieved through the use of an extract, transform and load (ETL) tool, which is programed to select data that matches a specific criteria and ensure data consistency before loading it into the business intelligence solution.

Data architecture is the process of designing internal tables and data structures. This is one of the more important business intelligence tools, because a well-designed system can expand capacity and grow to meet changing needs. A user interface provides the architect with the tools necessary to create and maintain these tables.

The purpose of business intelligence tools is to enable a range of forms of analysis that would not otherwise be possible. Options include creation of ad hoc, scheduled and standard reports across a range of disciplines. The functionality required to create these reports is an essential part of the application. Query development requires a combination of technical skills and a solid understanding of business process.

The very best tools are useful only in the hands of skilled staff members. The primary requirements for a position in business intelligence include post-secondary training in statistics, advanced mathematics or computer systems. Many employers expect a graduate degree in statistics or mathematics for management roles.

Business intelligence products are constantly changing, and a commitment to continuing education is necessary to be successful in this type of work. An employee can keep his or her skills current through online courses, seminars and discussions. These skills can be used to help evaluate the merits and potential applications of business intelligence tools.






Data Analysis
Analysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of highlighting useful information, suggesting conclusions, and supporting decision making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, in different business, science, and social science domains.

\section{Data Mining}

Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes. Business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. 

In statistical applications, some people divide data analysis into descriptive statistics, exploratory data analysis, and confirmatory data analysis. EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical or structural models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis.
Data Integration
Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. 
The term data analysis is sometimes used as a synonym for data modeling, which is unrelated to the subject of this article.
Sources of Data

Internal source
External sources
Personal Data
Data Collection
Data Validation


\section{Data Cleansing}

Data cleansing or data scrubbing is the act of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database. Used mainly in databases, the term refers to identifying incomplete, incorrect, inaccurate, irrelevant etc. parts of the data and then replacing, modifying or deleting this dirty data.

After cleansing, a data set will be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by different data dictionary definitions of similar entities in different stores, may have been caused by user entry errors, or may have been corrupted in transmission or storage.

Data cleansing differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at entry time, rather than on batches of data.

The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code) or fuzzy (such as correcting records that partially match existing, known records).


The process of data analysis

Data analysis is a process, within which several phases can be distinguished:
Data cleaning
Initial data analysis (assessment of data quality)
Main data analysis (answer the original research question)
Final data analysis (necessary additional analyses and report)



Commercial Databases

Knowledge and Data
Knowledge is derived from data.
Data dredging

Data dredging (data fishing, data snooping) is the inappropriate (sometimes deliberately so) use of data mining to uncover misleading relationships in data. These relationships may be valid within the test set but have no statistical significance in the wider population.

Database and SQL
Structured Query Language (SQL) 
Structured query language is a database computer language designed for managing data in relational database management systems (RDBMS), and originally based upon relational algebra. Its scope includes data insert, query, update and delete, schema creation and modification, and data access control. 

Queries
The most common operation in SQL is the query, which is performed with the declarative SELECT statement. SELECT retrieves data from one or more tables, or expressions. Standard SELECT statements have no persistent effects on the database. 

\section{Data definition}
The Data Definition Language (DDL) manages table and index structure. The most basic items of DDL are the CREATE, ALTER, RENAME, DROP and TRUNCATE statements:
Data manipulation

The Data Manipulation Language (DML) is the subset of SQL used to add, update and delete data:

Database administration
Database administration is a job whose primary function is the overall support of a computer database. These support tasks are performed by a person called a database administrator, or DBA. Databases require constant management and upkeep, and a DBA is specially trained to perform all of the functions necessary to do so. A DBA is normally required to have certification or a degree in supporting a specific type of database system, such as Oracle or Microsoft SQL Server. Typically, he or she will use a database management system, or DBMS, software package containing programs designed to aid in database administration.

There are numerous responsibilities involved when doing database administration. DBAs are generally in charge of the overall design, layout, and implementation of the database itself, and need to plan for any changes or future growth needed. They monitor performance of the database and related applications, tuning and making modifications as needed to ensure everything is working optimally. They establish and document database security policies and procedures, as well as those for backup and recovery. DBAs need to have a thorough understanding of database software, features, and products, how to troubleshoot them, and how to install, configure, and upgrade them.

Using a database management system software package greatly enhances the ability of the DBA to effectively support the database. These programs allow the data in the database to be easily managed, organized, and retrieved. They can interact with different types of database models, such as network or relational models. They provide a convenient means to query the data stored there, as well as an easy method for inserting, updating, and deleting records. Database management systems also help maintain data integrity and control access.

The three main variations of the basic database administration job include systems, development, and applications. Responsibility for all of the physical aspects of database administration, such as upgrades, backups, and performance monitoring and tuning typically fall under a systems DBA. Development DBAs are usually responsible for the activities involved in designing and implementing a new database. When a company uses software from an outside vendor to interact with the database, an applications DBA is typically in charge of ensuring they work together properly. A database administrator may specialize in one of these types, or may be responsible for all of them depending on the size of the organization and its needs.



\section{Decision Support Systems}
Decision support systems are software tools that are developed for the specific purpose of assimilating pertinent data and generating a detail analysis of a given situation. The results of the decision support system, or DSS, can then be utilized in making an informed decision regarding just about any action or set of circumstances.

Typical decision support systems are designed for easy use by end users who may or may not be comfortable with using computer technology as part of the decision making process. Often, the software will be make use of formats such as spreadsheets or databases that work with the use of fields to enter data. The process of entering data into the program is usually very straightforward, and often includes tips and other forms of assistance as the data is entered into the fields. Once the information is entered, it is possible to query the system with a series of questions that can allow the user to project not only a range of possible courses of action, but also get some idea on the possible outcomes associated with each option.

In terms of dealing with many managerial and executive decisions that are made daily in the workplace, decision support systems can be a great tool for supervisors, department heads, and site managers. The system can be configured to work within just about any area that involves dealing with labor issues. Human resource personnel can make use of decision support systems that include data about all applicable local and federal laws governing the rights and protections relevant to employees, making the process of administering promotions, dealing with a leave of absence, or developing an equitable approach to disciplinary action much easier. Supervisors can also use decision support systems as part of the identifying strengths and weaknesses among employees in their charge, which can take a lot of the guesswork out of performing period employee evaluations.

Over the last few years, decision support systems have become better known as Business Intelligence systems. However, many persons who have made use of the software for a number of years still tend to make use of the identification of decision support software, which has led many producers of the software to continue use of the DSS identification.

A decision support system is composed of the following subsystems.

Data management
Model Management
Knowledge management

\section{Database}
A database is a collection of interrelated data organized in such a way that it corresponds to the needs and strucuture of an organization and can be used by more than one person for more than one application.
Database Management System
Storage
Retrieval 
Control
Query Facility
