\section{Machine Learning: Recommender Systems \& Dimensionality Reduction}

%==========================================================%
\subsection{Case Study: Recommending Products}
\begin{itemize}
\item How does Amazon recommend products you might be interested in purchasing?  How does Netflix decide which movies or TV shows you might want to watch?  What if you are a new user, should Netflix just recommend the most popular movies?  Who might you form a new link with on Facebook or LinkedIn?  
\item These questions are endemic to most service-based industries, and underlie the notion of collaborative filtering and the recommender systems deployed to solve these problems.  In this fourth case study, you will explore these ideas in the context of recommending products based on customer reviews.  
\item 
In this course, you will explore dimensionality reduction techniques for modeling high-dimensional data.  In the case of recommender systems, your data is represented as user-product relationships, with potentially millions of users and hundred of thousands of products.  \item You will implement matrix factorization and latent factor models for the task of predicting new user-product relationships.  You will also use side information about products and users to improve predictions.
\end{itemize}
%==========================================================%
\subsection{Learning Outcomes}
By the end of this course, you will be able to:
\begin{itemize}
\item Create a collaborative filtering system.
\item Reduce dimensionality of data using SVD, PCA, and random projections.
\item Perform matrix factorization using coordinate descent.
\item Deploy latent factor models as a recommender system.
\item Handle the cold start problem using side information.
\item Examine a product recommendation application.
\item Implement these techniques in Python.
\end{itemize}

%==========================================================%
\subsection{Latent Variable Models}
A latent variable model is a statistical model that relates a set of variables (so-called manifest variables) to a set of latent variables.

It is assumed that the responses on the indicators or manifest variables are the result of an individual's position on the latent variable(s), and that the manifest variables have nothing in common after controlling for the latent variable (local independence).

\end{document}
