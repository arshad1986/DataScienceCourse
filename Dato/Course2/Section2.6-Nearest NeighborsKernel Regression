WEEK 6
Nearest Neighbors & Kernel Regression
Up to this point, we have focused on methods that fit parametric functions---like polynomials and hyperplanes---to the entire dataset. In this module, we instead turn our attention to a class of "nonparametric" methods. These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.
We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set. This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors. Building on this idea, we turn to kernel regression. Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point. You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle. You will also implement these techniques and observe their practical behavior.

\item Slides presented in this module
\item Limitations of parametric regression
\item 1-Nearest neighbor regression approach
\item Distance metrics
\item 1-Nearest neighbor algorithm
\item k-Nearest neighbors regression
\item k-Nearest neighbors in practice
\item Weighted k-nearest neighbors
\item From weighted k-NN to kernel regression
\item Global fits of parametric models vs. local fits of kernel regression
\item Performance of NN as amount of data grows
\item Issues with high-dimensions, data scarcity, and computational complexity
\item k-NN for classification
\item A brief recap
Quiz · Nearest Neighbors & Kernel Regression
\item Reading: Predicting house prices using k-nearest neighbors regression
Quiz · Predicting house prices using k-nearest neighbors regression
Closing Remarks
In the conclusion of the course, we will recap what we have covered. This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization. We also briefly discuss some important regression techniques we did not cover in this course.
We conclude with an overview of what's in store for you in the rest of the specialization.

\item Slides presented in this module
\item Simple and multiple regression
\item Assessing performance and ridge regression
\item Feature selection, lasso, and nearest neighbor regression
\item What we covered and what we didn't cover
\item What's ahead in the ML specialization
\item Thank you!
Hi
