WEEK 4
Ridge Regression
You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data. In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue. This method is called "ridge regression". You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions. To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective. You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression. To select the strength of the bias away from overfitting, you will explore a general-purpose method called "cross validation".
You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant.

\item Slides presented in this module
\item Symptoms of overfitting in polynomial regression
\item Download the notebook and follow along
\item Overfitting demo
\item Overfitting for more general multiple regression models
\item Balancing fit and magnitude of coefficients
\item The resulting ridge objective and its extreme solutions
\item How ridge regression balances bias and variance
\item Download the notebook and follow along
\item Ridge regression demo
\item The ridge coefficient path
\item Computing the gradient of the ridge objective
\item Approach 1: closed-form solution
\item Discussing the closed-form solution
\item Approach 2: gradient descent
\item Selecting tuning parameters via cross validation
\item K-fold cross validation
\item How to handle the intercept
\item A brief recap
Quiz · Ridge Regression
\item Reading: Observing effects of L2 penalty in polynomial regression
Quiz · Observing effects of L2 penalty in polynomial regression
\item Reading: Implementing ridge regression via gradient descent
Quiz · Implementing ridge regression via gradient descent
